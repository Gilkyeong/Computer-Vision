## ğŸŒ€ ë¬¸ì œ 1 ê°„ë‹¨í•œ ì´ë¯¸ì§€ ë¶„ë¥˜ê¸° êµ¬í˜„

> MNIST datasetsì„ ì´ìš©í•˜ì—¬ **MLP êµ¬í˜„**
---
**MLP(Multi Layer Perceptron)** <br><br>
![image](https://github.com/user-attachments/assets/87b523bb-a531-4def-bab3-38eec11c9a4b)

### ğŸ“„ ì½”ë“œ 
- MNIST_MLP.py

*ì „ì²´ ì½”ë“œ*
```python
import numpy as np
import tensorflow as tf
import tensorflow.keras.datasets as ds

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

(x_train,y_train),(x_test,y_test)=ds.mnist.load_data()
x_train=x_train.reshape(60000,784)
x_test=x_test.reshape(10000,784)
x_train=x_train.astype(np.float32)/255.0
x_test=x_test.astype(np.float32)/255.0
y_train=tf.keras.utils.to_categorical(y_train,10)
y_test=tf.keras.utils.to_categorical(y_test,10)

dmlp=Sequential()
dmlp.add(Dense(units=1024,activation='relu',input_shape=(784,)))
dmlp.add(Dense(units=512,activation='relu'))
dmlp.add(Dense(units=512,activation='relu'))
dmlp.add(Dense(units=10,activation='softmax'))

dmlp.compile(loss='categorical_crossentropy',optimizer=Adam(learning_rate=0.0001),metrics=['accuracy'])
hist=dmlp.fit(x_train,y_train,batch_size=128,epochs=30,validation_data=(x_test,y_test),verbose=2)
print('acc =', dmlp.evaluate(x_test,y_test,verbose=0)[1]*100)

import matplotlib.pyplot as plt

plt.plot(hist.history['accuracy'])
plt.plot(hist.history['val_accuracy'])
plt.title('Accuracy graph')
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.legend(['train','test'])
plt.grid()
plt.show()
```
*í•µì‹¬ì½”ë“œ* <br>
**ğŸ”· ë°ì´í„°ì…‹ ë¡œë“œ**
```python
(x_train,y_train),(x_test,y_test)=ds.mnist.load_data()
```
ğŸ”¹ tensorflow.keras.datasets ëª¨ë“ˆì„ ì‚¬ìš©í•˜ì—¬ MNIST ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜´ <br>
ğŸ”¹ MNIST : ì†ê¸€ì”¨ ìˆ«ì ì´ë¯¸ì§€(28Ã—28 í¬ê¸°), Train data : 60,000ê°œ Test data : 10,000ê°œë¡œ êµ¬ì„±
<br><br>
**ğŸ”· Flattening**
```python
x_train = x_train.reshape(60000, 784)
x_test = x_test.reshape(10000, 784)
```
ğŸ”¹ ê° ì´ë¯¸ì§€ëŠ” 28Ã—28 í–‰ë ¬ í˜•íƒœ <br>
ğŸ”¹ nn inputìœ¼ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ (60000, 784)ì™€ (10000, 784)ì˜ 1ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜
<br><br>
**ğŸ”· Normalization**
```python
x_train = x_train.astype(np.float32) / 255.0
x_test = x_test.astype(np.float32) / 255.0
```
ğŸ”¹ 0-1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ì •ê·œí™”
<br><br>
**ğŸ”· label One-Hot Encoding**
```python
y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test = tf.keras.utils.to_categorical(y_test, 10)
```
ğŸ”¹ 10ê°œì˜ í´ë˜ìŠ¤ í™•ë¥ ì„ ë°˜í™˜í•˜ë„ë¡ ë³€í™˜
<br><br>
**ğŸ”· Model êµ¬ì¡° ì„¤ê³„**
```python
dmlp=Sequential()
dmlp.add(Dense(units=1024,activation='relu',input_shape=(784,)))
dmlp.add(Dense(units=512,activation='relu'))
dmlp.add(Dense(units=512,activation='relu'))
dmlp.add(Dense(units=10,activation='softmax'))
```
ğŸ”¹ Kerasì˜ Sequential ëª¨ë¸ì„ ì‚¬ìš©í•´ layerë¥¼ ìŒ“ìŒ <br>
ğŸ”¹ Dense layer <br>
    ì²«ë²ˆì§¸ layer <br>
     Unit : 1024, activation function : relu, input : 784 <br>
    ë‘ë²ˆì§¸, ì„¸ë²ˆì§¸ layer <br>
     Unit : 512, activation function : relu <br>
    output layer <br>
     Unit : 10 (10ê°œ class), activation function : softmax (ê° í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥  ê°’ ì¶œë ¥)
<br><br>
**ğŸ”· Model ì»´íŒŒì¼**
```python
dmlp.compile(loss='categorical_crossentropy',optimizer=Adam(learning_rate=0.0001),metrics=['accuracy'])
hist=dmlp.fit(x_train,y_train,batch_size=128,epochs=30,validation_data=(x_test,y_test),verbose=2)
print('acc =', dmlp.evaluate(x_test,y_test,verbose=0)[1]*100)
```
ğŸ”¹ dmlp.compile : ëª¨ë¸ ì»´íŒŒì¼ <br>
   loss function : categorical_crossentropy, Optimizer : Adam, learning rate : 0.0001 <br>
ğŸ”¹ dmlp.fit : ëª¨ë¸ í•™ìŠµ <br>
   batch size : 128, epochs : 30, verbose : 2 (ê° epochsì— ëŒ€í•œ log ì¶œë ¥)
<br><br>
**ğŸ”· ëª¨ë¸ í‰ê°€**
```python
print('acc =', dmlp.evaluate(x_test, y_test, verbose=0)[1] * 100)
```
ğŸ”¹ Trainì´ ì™„ë£Œëœ í›„ test dataë¥¼ ì‚¬ìš©í•´ accuracy í‰ê°€ <br><br>
![ìŠ¤í¬ë¦°ìƒ· 2025-04-08 162656](https://github.com/user-attachments/assets/06d48ed4-ece3-415d-9d32-44c3b7f52a5b)
<br><br>
### :octocat: ì‹¤í–‰ ê²°ê³¼
![mlpacc](https://github.com/user-attachments/assets/44c99f1f-3298-4d63-84ab-726f35b49525)
<br><br>
## ğŸŒ€ ë¬¸ì œ 2 CIFAR-10 datasetsì„ í™œìš©í•œ CNN ëª¨ë¸ êµ¬ì¶•

> CIFAR-10 datasetsì„ í™œìš©í•˜ì—¬ **CNNì„ êµ¬ì¶•í•˜ê³  image classification ìˆ˜í–‰**
---
**CNN(Convolutional Neural Network)** <br><br>
![image](https://github.com/user-attachments/assets/d49fd4e0-cd3a-482e-9158-47d99863e7be)

### ğŸ“„ ì½”ë“œ 
- CIFAR10-CNN.py

*ì „ì²´ ì½”ë“œ*
```python
import numpy as np
import tensorflow as tf
import tensorflow.keras.datasets as ds
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import load_img, img_to_array

(x_train, y_train), (x_test, y_test) = ds.cifar10.load_data()
x_train = x_train.astype(np.float32) / 255.0
x_test = x_test.astype(np.float32) / 255.0
y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test = tf.keras.utils.to_categorical(y_test, 10)

cnn = Sequential()
cnn.add(Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)))
cnn.add(Conv2D(32, (3,3), activation='relu'))
cnn.add(MaxPooling2D(pool_size=(2,2)))
cnn.add(Dropout(0.25))
cnn.add(Conv2D(64, (3,3), activation='relu'))
cnn.add(Conv2D(64, (3,3), activation='relu'))
cnn.add(MaxPooling2D(pool_size=(2,2)))
cnn.add(Dropout(0.25))
cnn.add(Flatten())
cnn.add(Dense(512, activation='relu'))
cnn.add(Dropout(0.5))
cnn.add(Dense(10, activation='softmax'))

cnn.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])
hist = cnn.fit(x_train, y_train,
               batch_size=128, epochs=50,
               validation_data=(x_test, y_test),
               verbose=2)

res = cnn.evaluate(x_test, y_test, verbose=0)
print('acc =', res[1]*100)

plt.plot(hist.history['accuracy'])
plt.plot(hist.history['val_accuracy'])
plt.title('Accuracy graph')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'])
plt.grid()
plt.show()

test_img = load_img("dog.jpg", target_size=(32,32))
test_img_array = img_to_array(test_img)
test_img_array = test_img_array.astype(np.float32) / 255.0
test_img_array = np.expand_dims(test_img_array, axis=0)

prediction = cnn.predict(test_img_array)
predicted_class_idx = np.argmax(prediction, axis=1)[0]

class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 
               'dog', 'frog', 'horse', 'ship', 'truck']
print("Index :", predicted_class_idx)
print("Class name :", class_names[predicted_class_idx])
```

*í•µì‹¬ ì½”ë“œ* <br>
**ğŸ”· ë°ì´í„°ì…‹ ë¡œë“œ**
```python
(x_train, y_train), (x_test, y_test) = ds.cifar10.load_data()
```
ğŸ”¹ tensorflow.keras.datasets ëª¨ë“ˆì˜ cifar10 ë°ì´í„°ì…‹ì„ ì‚¬ìš© <br>
ğŸ”¹ CIFAR-10 : 32Ã—32 í¬ê¸°ì˜ 60,000ê°œ ì»¬ëŸ¬ ì´ë¯¸ì§€, 10ê°œì˜ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜
<br><br>
**ğŸ”· Normalization**
```python
x_train = x_train.astype(np.float32) / 255.0
x_test = x_test.astype(np.float32) / 255.0
```
ğŸ”¹ 0-1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ì •ê·œí™”
<br><br>
**ğŸ”· Label One-Hot Encoding**
```python
y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test = tf.keras.utils.to_categorical(y_test, 10)
```
ğŸ”¹ 10ê°œì˜ í´ë˜ìŠ¤ í™•ë¥ ì„ ë°˜í™˜í•˜ë„ë¡ ë³€í™˜
<br><br>
**ğŸ”· CNN ëª¨ë¸ ì„¤ê³„**
```python
cnn = Sequential()
cnn.add(Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)))
cnn.add(Conv2D(32, (3,3), activation='relu'))
cnn.add(MaxPooling2D(pool_size=(2,2)))
cnn.add(Dropout(0.25))
cnn.add(Conv2D(64, (3,3), activation='relu'))
cnn.add(Conv2D(64, (3,3), activation='relu'))
cnn.add(MaxPooling2D(pool_size=(2,2)))
cnn.add(Dropout(0.25))
cnn.add(Flatten())
cnn.add(Dense(512, activation='relu'))
cnn.add(Dropout(0.5))
cnn.add(Dense(10, activation='softmax'))
```
ğŸ”¹ Kerasì˜ Sequential ëª¨ë¸ì„ ì‚¬ìš©í•´ layerë¥¼ ìŒ“ìŒ <br>
```python
cnn.add(Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)))
cnn.add(Conv2D(32, (3,3), activation='relu'))
cnn.add(MaxPooling2D(pool_size=(2,2)))
cnn.add(Dropout(0.25))
```
ğŸ”¹ ì²«ë²ˆì§¸ Convolution layer <br>
    Conv2D layer <br>
     32ê°œ fliter, kernel size : 3x3, activation function : relu, input : (32, 32, 3) â†’ CIFAR-10 ì´ë¯¸ì§€ í¬ê¸°ì™€ ì±„ë„ ìˆ˜ <br>
    MaxPooling2D <br>
     2Ã—2 í’€ë§ì„ ì‚¬ìš©í•´ ê³µê°„ì  í¬ê¸°ë¥¼ ì¤„ì—¬ ê³„ì‚°ëŸ‰ ê°ì†Œ ë° íŠ¹ì§• ì¶”ì¶œ <br>
    Dropout(0.25) <br>
     Overfittingì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ëœë¤í•˜ê²Œ 25%ì˜ ë‰´ëŸ°ì„ ì œê±° <br>
```python
cnn.add(Conv2D(64, (3,3), activation='relu'))
cnn.add(Conv2D(64, (3,3), activation='relu'))
cnn.add(MaxPooling2D(pool_size=(2,2)))
cnn.add(Dropout(0.25))
```
ğŸ”¹ ë‘ë²ˆì§¸ Convolution layer <br>
    Conv2D layer <br>
     64ê°œ fliter, kernel size : 3x3, activation function : relu <br>
    MaxPooling2D <br>
     2Ã—2 í’€ë§ì„ ì‚¬ìš©í•´ ê³µê°„ì  í¬ê¸°ë¥¼ ì¤„ì—¬ ê³„ì‚°ëŸ‰ ê°ì†Œ ë° íŠ¹ì§• ì¶”ì¶œ <br>
    Dropout(0.25) <br>
     Overfittingì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ëœë¤í•˜ê²Œ 25%ì˜ ë‰´ëŸ°ì„ ì œê±° <br>
```python
cnn.add(Flatten())
cnn.add(Dense(512, activation='relu'))
cnn.add(Dropout(0.5))
cnn.add(Dense(10, activation='softmax'))
```
ğŸ”¹ FC layer <br>
    Flatten <br>
     2ì°¨ì› í˜•íƒœì˜ feature mapì„ 1ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜ <br>
    ì²«ë²ˆì§¸ Dense layer <br>
     Unit : 512, activation function : relu <br>
    Dropout(0.5) <br>
     Overfittingì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ëœë¤í•˜ê²Œ 50%ì˜ ë‰´ëŸ°ì„ ì œê±° <br>
    ë‘ë²ˆì§¸ Dense layer <br>
     Unit : 10, activation function : softmax <br>
<br><br>
**ğŸ”· Model ì»´íŒŒì¼**
```python
cnn.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])
hist = cnn.fit(x_train, y_train,
               batch_size=128, epochs=50,
               validation_data=(x_test, y_test),
               verbose=2)
```
ğŸ”¹ cnn.compile : ëª¨ë¸ ì»´íŒŒì¼ <br>
   loss function : categorical_crossentropy, Optimizer : Adam, learning rate : 0.001 <br>
ğŸ”¹ cnn.fit : ëª¨ë¸ í•™ìŠµ <br>
   batch size : 128, epochs : 50, verbose : 2 (ê° epochsì— ëŒ€í•œ log ì¶œë ¥)
<br><br>
**ğŸ”· ì •í™•ë„ ì¸¡ì •**
```python
res = cnn.evaluate(x_test, y_test, verbose=0)
print('acc =', res[1]*100)
```
ğŸ”¹ Trainì´ ì™„ë£Œëœ í›„ test dataë¥¼ ì‚¬ìš©í•´ accuracy í‰ê°€ <br><br>
![image](https://github.com/user-attachments/assets/9f5199c4-ff25-44ed-8169-fe9229ab754f)
<br><br>
**ğŸ”· ì´ë¯¸ì§€ classification**
```python
test_img = load_img("dog.jpg", target_size=(32,32))
test_img_array = img_to_array(test_img)
test_img_array = test_img_array.astype(np.float32) / 255.0
test_img_array = np.expand_dims(test_img_array, axis=0)

prediction = cnn.predict(test_img_array)
predicted_class_idx = np.argmax(prediction, axis=1)[0]

class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 
               'dog', 'frog', 'horse', 'ship', 'truck']
print("Index :", predicted_class_idx)
print("Class name :", class_names[predicted_class_idx])
```
ğŸ”¹ dog.jpgë¥¼ ë¶ˆëŸ¬ì™€ì„œ í•™ìŠµëœ cnn ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ classification<br>
ë¶ˆëŸ¬ì˜¨ ì´ë¯¸ì§€ <br>
![dog](https://github.com/user-attachments/assets/37d0e5c0-83b9-4296-8987-22578bf6111d) <br>
ì„±ëŠ¥ <br>
![ìŠ¤í¬ë¦°ìƒ· 2025-04-08 172139](https://github.com/user-attachments/assets/ac32b121-21ea-4102-8a6e-6d48c84a8112)
<br><br>
### :octocat: ì‹¤í–‰ ê²°ê³¼
![image](https://github.com/user-attachments/assets/d9c19ef9-f72d-4327-afc3-117acd25c23e)
<br><br>

## ğŸŒ€ ë¬¸ì œ 3 ì „ì´ í•™ìŠµì„ í™œìš©í•œ ì´ë¯¸ì§€ ë¶„ë¥˜ê¸° ê°œì„ 

> ì‚¬ì „ì— í•™ìŠµëœ ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ì „ì´ í•™ìŠµì„ ìˆ˜í–‰í•˜ê³  ì´ë¯¸ì§€ ë¶„ë¥˜ê¸°ì˜ ì„±ëŠ¥ í–¥ìƒ
---

### ğŸ“„ ì½”ë“œ 
- Trans_classsification.py

*ì „ì²´ ì½”ë“œ*
```python
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.applications import VGG16
from tensorflow.keras.datasets import cifar10

gpus = tf.config.list_physical_devices('GPU')

(x_train, y_train), (x_test, y_test) = cifar10.load_data()

def preprocess(x, y):
    x = tf.image.resize(x, [224, 224]) / 255.0
    return x, y

BATCH_SIZE = 16

train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_ds = train_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)
train_ds = train_ds.shuffle(5000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))
test_ds = test_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)
test_ds = test_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
base_model.trainable = False

model = models.Sequential([
    base_model,
    layers.Flatten(),
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit(train_ds, epochs=50, validation_data=test_ds)

test_loss, test_acc = model.evaluate(test_ds)
print(f"\n acc : {test_acc:.4f}")
```

*í•µì‹¬ ì½”ë“œ* <br>
**ğŸ”· ë°ì´í„°ì…‹ ë¡œë“œ**
```python
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

def preprocess(x, y):
    x = tf.image.resize(x, [224, 224]) / 255.0
    return x, y
```
ğŸ”¹ VGG16ì˜ inputì— ë§ì¶”ê¸° ìœ„í•´ 224x224ë¡œ resize í›„ 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
<br><br>
**ğŸ”· VGG16 base model ë¡œë“œ**
```python
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
base_model.trainable = False
```
ğŸ”¹ ImageNetì— ëŒ€í•´ ì‚¬ì „ í•™ìŠµëœ VGG16 ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜´ <br>
ğŸ”¹ include_top=Falseë¥¼ í†µí•´ VGG16 ëª¨ë¸ì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ì€ ì œê±°í•˜ê³  input size 224Ã—224ë¡œ ë§ì¶¤ <br>
ğŸ”¹ base_model.trainable = Falseë¡œ ì‚¬ì „ í•™ìŠµëœ weightê°€ í•™ìŠµ ì¤‘ì— ì—…ë°ì´íŠ¸ë˜ì§€ ì•Šë„ë¡ freeze <br>
<br><br>
**ğŸ”· ìƒˆë¡œìš´ layer ì„¤ê³„**
```python
model = models.Sequential([
    base_model,
    layers.Flatten(),
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(10, activation='softmax')
])
```
ğŸ”¹ Flatten layer : 2ì°¨ì› feature mapì„ 1ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜ <br>
ğŸ”¹ ì²«ë²ˆì§¸ Dense layer : Unit : 256, activation function : relu <br>
ğŸ”¹ Dropout (0.5): Overfittingì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ëœë¤í•˜ê²Œ 50%ì˜ ë‰´ëŸ°ì„ ì œê±° <br>
ğŸ”¹ ë‘ë²ˆì§¸ Dense layer : Unit : 10, activation function : softmax
<br><br>
**ğŸ”· Model ì»´íŒŒì¼**
```python
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
history = model.fit(train_ds, epochs=50, validation_data=test_ds)
```
ğŸ”¹ model.compile : ëª¨ë¸ ì»´íŒŒì¼ <br>
   loss function : sparse_categorical_crossentropy, Optimizer : Adam <br>
ğŸ”¹ model.fit : ëª¨ë¸ í•™ìŠµ <br>
   epochs : 50
<br><br>
**ğŸ”· ì •í™•ë„ ì¸¡ì •**
```python
test_loss, test_acc = model.evaluate(test_ds)
print(f"\n acc : {test_acc:.4f}")
```
ğŸ”¹ Trainì´ ì™„ë£Œëœ í›„ test dataë¥¼ ì‚¬ìš©í•´ accuracy í‰ê°€ <br><br>
<br><br>
### :octocat: ì‹¤í–‰ ê²°ê³¼

